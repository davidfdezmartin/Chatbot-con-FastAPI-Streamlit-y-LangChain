{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidfdezmartin/Chatbot-con-FastAPI-Streamlit-y-LangChain/blob/main/documentos_y_matcomplement/Colab%20de%20Pruebas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avPoFH9T_X8B"
      },
      "source": [
        "Aquí monto requeriments2.txt del GitHub pero com la libreria typer==0.09.0, que por un lado debe ser <0.10.00 (por eso he puesto 0.09.0 y por otro mayor que para fastapi-cli 0.0.4 requires typer>=0.12.3, a REVISARLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeKeYrcC8o2z"
      },
      "source": [
        "Si se ha instalado longchain y falla, quitar el # del comentario de esta celda.\n",
        "Uso rutas absolutas respecto al GitHub al principio para que no falle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkaYUOpwZ-d6",
        "outputId": "6833a08e-76be-4c72-d05c-11eddc6a35c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: langchain-community==0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: python-dotenv in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: streamlit in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: bs4 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 5)) (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from -r ../requirements2.txt (line 6)) (4.12.3)\n",
            "Requirement already satisfied: pypdf in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 7)) (4.2.0)\n",
            "Requirement already satisfied: faiss-cpu in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 8)) (1.8.0)\n",
            "Requirement already satisfied: groq in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 9)) (0.8.0)\n",
            "Requirement already satisfied: fastapi in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 10)) (0.110.0)\n",
            "Requirement already satisfied: uvicorn in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 11)) (0.28.0)\n",
            "Requirement already satisfied: sse_starlette in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 12)) (2.1.0)\n",
            "Requirement already satisfied: googletrans in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 13)) (2.4.0)\n",
            "Requirement already satisfied: typer==0.09.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: wikipedia-api in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 15)) (0.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.6.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.26.3)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (8.3.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from typer==0.09.0->-r ../requirements2.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from typer==0.09.0->-r ../requirements2.txt (line 14)) (4.10.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (5.3.3)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (10.2.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (4.25.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (16.1.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (13.7.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (6.4)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (4.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->-r ../requirements2.txt (line 6)) (2.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from groq->-r ../requirements2.txt (line 9)) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from groq->-r ../requirements2.txt (line 9)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from groq->-r ../requirements2.txt (line 9)) (0.26.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from groq->-r ../requirements2.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from fastapi->-r ../requirements2.txt (line 10)) (0.36.3)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn->-r ../requirements2.txt (line 11)) (0.14.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (4.21.1)\n",
            "Requirement already satisfied: toolz in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->groq->-r ../requirements2.txt (line 9)) (3.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->groq->-r ../requirements2.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: colorama in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer==0.09.0->-r ../requirements2.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r ../requirements2.txt (line 4)) (4.0.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->groq->-r ../requirements2.txt (line 9)) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->groq->-r ../requirements2.txt (line 9)) (1.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.10.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->-r ../requirements2.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from rich<14,>=10.14.0->streamlit->-r ../requirements2.txt (line 4)) (2.17.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r ../requirements2.txt (line 4)) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (0.17.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r ../requirements2.txt (line 4)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#  %pip uninstall -y langchain langchain-community\n",
        "%pip install -r \"../requirements2.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3ulmY2na_zL"
      },
      "source": [
        "SI HACE FALTA WIKI-LANGCHAIN. En celdas posteriores se hacen pruebas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDeVvW7iXd0z",
        "outputId": "ac96d7c0-1b24-497b-dc64-cf2d9d4a0516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet wikipedia langchain langchain_community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNPGlxA3ar9Q"
      },
      "source": [
        "AGENTE DE USUARIO PARA WIKIPEDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) Hago una prueba pidiendole a Wiki DIABETES PRIMERO y funciona sin problema. \n",
        "\n",
        "2) Luego hago una prueba de idioma para ver si tb funciona en español con una búsqueda multiple de términos\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"Cáncer\", \"Hipertensión\"]\n",
        "\n",
        "Cambios probados:\n",
        "Idioma del API de Wikipedia Y PRUEBO UNA BUSQUEDA MULTIPLE:\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "Mensaje de error en español:\n",
        "\n",
        "return \"No se encontró la página para la consulta: \" + query\n",
        "Estos cambios aseguran que las consultas a Wikipedia se realicen en la versión en español y que cualquier mensaje de error también esté en español.\n",
        "\n",
        "RESULTADO: Busca en la wiki en ingles, por lo que los terminos que no son iguales en los 2 idiomas, no los encuentra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrTAmQg-avk8",
        "outputId": "99de8728-f0a0-4d63-8e5e-a7517f7e1a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado para 'Diabetes':\n",
            "Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects. Classic symptoms include thirst, polyuria, weight loss, and blurred vision. If left untreated, the disease can lead to various health complications, including disorders of the cardiovascular system, eye, kidney, and nerves. Untreated or poorly treated diabetes accounts for approximately 1.5 million deaths every year.\n",
            "The major types of diabetes are type 1 and type 2. The most common treatment for type 1 is insulin replacement therapy (insulin injections), while anti-diabetic medications (such as metformin and semaglutide) and lifestyle modifications can be used to manage type 2. Gestational diabetes, a form that arises during pregnancy in some women, normally resolves shortly after delivery.\n",
            "As of 2021, an estimated 537 million people had diabetes worldwide accounting for 10.5% of the adult population, with type 2 making up about 90% of all cases. It is estimated that by 2045, approximately 783 million adults, or 1 in 8, will be living with diabetes, representing a 46% increase from the current figures. The prevalence of the disease continues to increase, most dramatically in low- and middle-income nations. Rates are similar in women and men, with diabetes being the seventh leading cause of death globally. The global expenditure on diabetes-related healthcare is an estimated US$760 billion a year.\n",
            "\n",
            "Resultado para 'Hemocromatosis':\n",
            "No se encontró la página para la consulta: Hemocromatosis\n",
            "\n",
            "Resultado para 'Cáncer':\n",
            "No se encontró la página para la consulta: Cáncer\n",
            "\n",
            "Resultado para 'Hipertensión':\n",
            "No se encontró la página para la consulta: Hipertensión\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No se encontró la página para la consulta: \" + query\n",
        "\n",
        "# Ejemplo de uso con varios términos en español\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"Cáncer\", \"Hipertensión\"]\n",
        "for query in queries:\n",
        "    result = get_wikipedia_summary(query)\n",
        "    print(f\"Resultado para '{query}':\\n{result}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPTCzk1ebdf6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l9vWb27cmKH"
      },
      "source": [
        "AHORA INTEGRAMOS LANGCHAIN CON WIKI Y PROBAMOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXS3LQ21deFF",
        "outputId": "e7bee3c7-9ccb-4ed4-eca4-2eb692c88e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: tqdm in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (1.26.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (0.23.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (10.2.0)\n",
            "Requirement already satisfied: filelock in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2021.4.0)\n",
            "Requirement already satisfied: colorama in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence_transformers) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence_transformers) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA-C3lf_esDR"
      },
      "source": [
        "PRUEBA DE API KEY DE HUGGING FACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztVtxmQIAirG"
      },
      "source": [
        "Me funciona muy bien subiendo el .env de GitHub al root de aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdO1rIukertP",
        "outputId": "02542965-9cba-4b42-e207-53ad0cd861ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to C:\\Users\\casti\\.cache\\huggingface\\token\n",
            "Login successful\n",
            "GROQ_API_KEY cargado correctamente\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "add_to_git_credential=True\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    from huggingface_hub import login\n",
        "    login(hf_token)\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Obtener otras variables de entorno si es necesario\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(\"GROQ_API_KEY cargado correctamente\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY no encontrado en el archivo .env\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todo sale bien, pero si se quiere quitar el aviso en rojo debe ponerse en un .sh o correr en la terminal: git config --global credential.helper store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to C:\\Users\\casti\\.cache\\huggingface\\token\n",
            "Login successful\n",
            "HUGGING_FACE_API_TOKEN cargado correctamente\n",
            "GROQ_API_KEY cargado correctamente\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    from huggingface_hub import login\n",
        "    login(token=hf_token, add_to_git_credential=True)  # Agregar el parámetro aquí\n",
        "    print(\"HUGGING_FACE_API_TOKEN cargado correctamente\")\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Obtener otras variables de entorno si es necesario\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(\"GROQ_API_KEY cargado correctamente\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY no encontrado en el archivo .env\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pg-fC0e56F"
      },
      "source": [
        "AHORA PRUEBA DE CODIGO BUENO\n",
        "Explicación\n",
        "VectorStoreRetriever: Se utiliza para configurar el recuperador basado en el vectorstore.\n",
        "load_qa_chain: Carga una cadena de preguntas y respuestas utilizando el PromptTemplate.\n",
        "RetrievalQA: Se configura con el retriev:\n",
        "D:\\GitHub\\TFM\\Chatbot-con-FastAPI-Streamlit-y-LangChain-1\\documentos_y_matcomplement\\docuentreno\\md\\Cholesterol-Myths-vs-Facts-Spanish.md\n",
        "\n",
        "Pruebo con un pdf pasado a .md de David En PDF\n",
        "\n",
        "RESULTADO: LO CARGA bien, en MD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JevHfNjqf3TG"
      },
      "outputs": [],
      "source": [
        "CARGAR=\"D:\\GitHub\\TFM\\Chatbot-con-FastAPI-Streamlit-y-LangChain-1\\documentos_y_matcomplement\\docuentreno\\md\\Cholesterol-Myths-vs-Facts-Spanish.md\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PRUEBA GENERAL CON WIKI Y LANGCHAIN: USAMOS MODELO DE HUGGING FACE GPT2.\n",
        "Creamos una variable para poder cambiar el numero maximo de tokens de entrada y salida si cambiamos el modelo facilmente.\n",
        "\n",
        "Explicación.\n",
        "a) Definir el límite de tokens: Se establece MAX_INPUT_TOKENS para los tokens de entrada y MAX_NEW_TOKENS para los nuevos tokens generados.\n",
        "\n",
        "b) Creamos una función para recortar el contexto: truncate_context recorta el contexto para asegurarse de que no se exceda el límite de tokens.\n",
        "\n",
        "c) Crear el prompt con el contexto recortado: Se asegura de que el prompt total no exceda el límite de tokens permitido por el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explicación de las funciones de control de tokens\n",
        "\n",
        "Hemos implementado funciones de control de tokens en nuestro código para asegurar que el número total de tokens (entrada más salida) no exceda el límite permitido por el modelo `gpt2`. Aquí está la explicación de las funciones clave que hemos creado y cómo puedes ajustarlas cuando cambies de modelo.\n",
        "\n",
        "#### Función `truncate_context`\n",
        "\n",
        "Esta función se utiliza para recortar el contexto de entrada de modo que el número total de tokens no exceda el límite permitido. Utilizamos el `GPT2Tokenizer` para contar y recortar los tokens.\n",
        "\n",
        "**Parámetros:**\n",
        "- `context`: El texto de contexto que se desea truncar.\n",
        "- `max_tokens`: El número máximo de tokens permitidos para el contexto.\n",
        "\n",
        "**Descripción:**\n",
        "- La función tokeniza el contexto usando `tokenizer.encode`.\n",
        "- Si el número de tokens excede `max_tokens`, se truncan los tokens al límite especificado.\n",
        "- La función devuelve el contexto truncado como texto utilizando `tokenizer.decode`.\n",
        "\n",
        "#### Definición de límites de tokens\n",
        "\n",
        "Hemos definido los límites de tokens de entrada (`MAX_INPUT_TOKENS`) y salida (`MAX_NEW_TOKENS`) para asegurarnos de que el número total de tokens no exceda 1024, el límite para `gpt2`.\n",
        "**Descripción:**\n",
        "- `MAX_INPUT_TOKENS`: El número máximo de tokens permitidos para el contexto de entrada.\n",
        "- `MAX_NEW_TOKENS`: El número máximo de tokens permitidos para la respuesta generada por el modelo.\n",
        "\n",
        "#### Aplicación en el código\n",
        "\n",
        "Utilizamos la función `truncate_context` para recortar el contexto antes de crear el prompt y enviar la solicitud al modelo.\n",
        "\n",
        "### Ajustes para cambiar de modelo\n",
        "\n",
        "Cuando se cambie de modelo, necesitaremos ajustar los límites de tokens (`MAX_INPUT_TOKENS` y `MAX_NEW_TOKENS`) según las especificaciones del nuevo modelo. La mayoría de los modelos tienen documentación que indica el límite máximo de tokens que pueden manejar. Por ejemplo, si cambia a un modelo con un límite de 2048 tokens, podrías ajustar los límites de la siguiente manera:\n",
        "\n",
        "```python\n",
        "MAX_INPUT_TOKENS = 1948  # Ajustar según el nuevo límite\n",
        "MAX_NEW_TOKENS = 100\n",
        "```\n",
        "\n",
        "Asegurarse de revisar la documentación del nuevo modelo para conocer el límite exacto de tokens y ajustar las variables en consecuencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado para 'Diabetes':\n",
            "Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects. Classic symptoms include thirst, polyuria, weight loss, and blurred vision. If left untreated, the disease can lead to various health complications, including disorders of the cardiovascular system, eye, kidney, and nerves. Untreated or poorly treated diabetes accounts for approximately 1.5 million deaths every year.\n",
            "The major types of diabetes are type 1 and type 2. The most common treatment for type 1 is insulin replacement therapy (insulin injections), while anti-diabetic medications (such as metformin and semaglutide) and lifestyle modifications can be used to manage type 2. Gestational diabetes, a form that arises during pregnancy in some women, normally resolves shortly after delivery.\n",
            "As of 2021, an estimated 537 million people had diabetes worldwide accounting for 10.5% of the adult population, with type 2 making up about 90% of all cases. It is estimated that by 2045, approximately 783 million adults, or 1 in 8, will be living with diabetes, representing a 46% increase from the current figures. The prevalence of the disease continues to increase, most dramatically in low- and middle-income nations. Rates are similar in women and men, with diabetes being the seventh leading cause of death globally. The global expenditure on diabetes-related healthcare is an estimated US$760 billion a year.\n",
            "\n",
            "Resultado para 'Hemocromatosis':\n",
            "No se encontró la página para la consulta: Hemocromatosis\n",
            "\n",
            "Resultado para 'Cáncer':\n",
            "No se encontró la página para la consulta: Cáncer\n",
            "\n",
            "Resultado para 'Hipertensión':\n",
            "No se encontró la página para la consulta: Hipertensión\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n",
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `RetrievalQA` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use create_retrieval_chain instead.\n",
            "  warn_deprecated(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2310 > 1024). Running this sequence through the model will result in indexing errors\n",
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA Result:\n",
            " Responde la pregunta basándote en el siguiente contexto: ﻿<a name=\"br1\"></a> \n",
            "\n",
            "COLESTEROL: MITOS Y VERDADES\n",
            "\n",
            "Mito: Sólo necesitará analizar sus niveles de colesterol cuando tenga más de 30 años.\n",
            "\n",
            "Verdad: La Asociación Americana del Corazón (AHA) recomienda analizar los niveles de colesterol\n",
            "\n",
            "una vez entre los 9 y 11 años, y de nuevo entre los 17 y 21 años para niños y adultos jóvenes sin otros\n",
            "\n",
            "factores de riesgos o un historial familiar de enfermedad cardíaca temprana. Después de los 20 factores de riesgos o un historial familiar de enfermedad cardíaca temprana. Después de los 20\n",
            "\n",
            "años, su doctor volverá a analizar su colesterol y otros factores de riesgo cada cuatro a seis años\n",
            "\n",
            "siempre y cuando su riesgo se mantenga bajo.\n",
            "\n",
            "Mito: Sólo la gente con sobrepeso u obesidad tiene colesterol alto.\n",
            "\n",
            "Verdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso Verdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso\n",
            "\n",
            "u obesidad aumenta sus probabilidades de tener colesterol alto, pero ser delgado no lo protege.\n",
            "\n",
            "Independientemente de su peso, dieta y nivel de actividad física, debería medir su nivel de\n",
            "\n",
            "colesterol con regularidad.\n",
            "\n",
            "Mito: Tener colesterol alto es un problema sólo de los hombres.\n",
            "\n",
            "Verdad: Aunque la ateroesclerosis generalmente ocurre a una mayor edad en las mujeres que Verdad: Aunque la ateroesclerosis generalmente ocurre a una mayor edad en las mujeres que\n",
            "\n",
            "en los hombres, la enfermedad cardiovascular sigue siendo la principal causa de muerte en las\n",
            "\n",
            "mujeres. Los médicos deben considerar condiciones medicas especiﬁcas de mujeres, tales como\n",
            "\n",
            "menopausia prematura (antes de los 40 años) y condiciones asociadas al embarazo, cuando les\n",
            "\n",
            "hablen sobre sus niveles de colesterol y las opciones para su tratamiento. hablen sobre sus niveles de colesterol y las opciones para su tratamiento.\n",
            "\n",
            "Mito: Si su médico no ha mencionado sus niveles de colesterol, usted está bien.\n",
            "\n",
            "Verdad: Usted puede hacerse cargo de su salud. Si usted tiene 20 años o más, pídale a su médico\n",
            "\n",
            "que le realice un análisis de colesterol, que evalúe sus factores de riesgo y que determine su riesgo\n",
            "\n",
            "de tener un ataque al corazón o un derrame cerebral. Si tiene entre 20 y 39 años, su médico debería evaluar su ries\n",
            "Pregunta: ¿Cuál es el tema principal del documento? ¿Público, en muy de la meretico\n",
            "\n",
            "seo puede los niveles de colesterol por cada del políticas, que la quesar el\n",
            "\n",
            "municiones a la país, una salud el tema principal del documento? ¿Cuál es el tema principal del documento?\n",
            "\n",
            "Sería: Público se muy en las hombres\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Usar root como base del GitHub\n",
        "root = \"https://github.com/davidfdezmartin/Chatbot-con-FastAPI-Streamlit-y-LangChain/blob/main\"\n",
        "\n",
        "# Verificar y obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No se encontró la página para la consulta: \" + query\n",
        "\n",
        "# Ejemplo de uso con varios términos en español\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"Cáncer\", \"Hipertensión\"]\n",
        "for query in queries:\n",
        "    result = get_wikipedia_summary(query)\n",
        "    print(f\"Resultado para '{query}':\\n{result}\\n\")\n",
        "\n",
        "# URL del archivo en GitHub\n",
        "file_url = f\"{root}/documentos_y_matcomplement/docuentreno/md/Cholesterol-Myths-vs-Facts-Spanish.md?raw=true\"\n",
        "\n",
        "# Descargar el contenido del archivo\n",
        "response = requests.get(file_url)\n",
        "if response.status_code != 200:\n",
        "    raise RuntimeError(f\"Error al descargar el archivo desde {file_url}\")\n",
        "\n",
        "# Guardar el contenido en un archivo temporal\n",
        "with open(\"temp.md\", \"w\", encoding=\"utf-8\") as temp_file:\n",
        "    temp_file.write(response.text)\n",
        "\n",
        "# Cargar el documento .md desde el archivo temporal\n",
        "text_loader = TextLoader(\"temp.md\")\n",
        "documents = text_loader.load()\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tamaño del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta basándote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Definir el límite de tokens\n",
        "MAX_INPUT_TOKENS = 824  # Ajustar según sea necesario\n",
        "MAX_NEW_TOKENS = 100\n",
        "TOTAL_TOKENS = MAX_INPUT_TOKENS + MAX_NEW_TOKENS\n",
        "\n",
        "# Inicializar el tokenizer de GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Función para recortar el contexto si es necesario\n",
        "def truncate_context(context, max_tokens):\n",
        "    tokens = tokenizer.encode(context)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        return tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
        "    return context\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas con control de longitud de tokens\n",
        "question = \"¿Cuál es el tema principal del documento?\"\n",
        "\n",
        "# Obtener el contexto\n",
        "context = \" \".join([doc.page_content for doc in texts])\n",
        "context = truncate_context(context, MAX_INPUT_TOKENS)\n",
        "\n",
        "# Crear el prompt con el contexto recortado\n",
        "prompt = prompt_template.format(context=context, question=question)\n",
        "\n",
        "# Generar la respuesta utilizando el modelo de lenguaje\n",
        "result = llm(prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------REVISADO HASTA AQUÍ EL 28/05 14------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-o4e4A-1Y28"
      },
      "source": [
        "PRUEBA 2. ASI CARGARIAMOS LOS PDF DE SU SUBDIRECTORIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "PXxZNfk41QeH",
        "outputId": "6b1fd0db-5df7-4edc-85e0-e7cf8cb3367f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'PyPDFDirectoryLoader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a3bbfe67377>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read the ppdfs from the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPyPDFDirectoryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PyPDFDirectoryLoader' is not defined"
          ]
        }
      ],
      "source": [
        "## Read the ppdfs from the folder\n",
        "loader=PyPDFDirectoryLoader(\"./pdf\")\n",
        "\n",
        "documents=loader.load()\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "\n",
        "final_documents=text_splitter.split_documents(documents)\n",
        "final_documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkt3WOT4EGyD"
      },
      "source": [
        "La anterior PRUEBA 1 (de PDF modificado)., lo rehago para cargar .md, y además sigo haciendo pruebas con WIKIPEDIA que fuciona perfecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUj79yYxHeeg"
      },
      "source": [
        "El error normal de longitud indica que el número de tokens en la entrada más los tokens generados exceden el límite de 1024 para el modelo gpt2. Para resolver esto, podemos:\n",
        "\n",
        "Reducir el tamaño del contexto enviado al modelo.\n",
        "Utilizar un modelo con un límite de tokens mayor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljP-ISvlEF5-",
        "outputId": "dbd1090b-23a2-4c0e-ee93-f58088f0975c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token de Hugging Face cargado correctamente.\n",
            "Wikipedia Result:\n",
            " Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tankōbon volumes as of November 2022. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\n",
            "Hunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\n",
            "The manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim's Toonami programming block from April 2016 to June 2019.\n",
            "Hunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA Result:\n",
            " {'query': '¿Cuál es el tema principal del documento?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nde calorías. Además, tenga en cuenta que la porción en que se basan esos números puede ser menor\\n\\nque el contenido completo del envase. (Es decir, puede que el envase incluya más de una porción).\\n\\nMito: Cambiar la mantequilla por margarina me ayudará a bajar el colesterol.\\n\\nVerdad: No necesariamente. La mantequilla es alta en grasa saturada, pero algunos tipos de\\n\\nmargarinas tienen incluso un contenido mayor de ambos tipos de grasas. Las margarinas líquidas\\n\\nfactores de riesgos o un historial familiar de enfermedad cardíaca temprana. Después de los 20\\n\\naños, su doctor volverá a analizar su colesterol y otros factores de riesgo cada cuatro a seis años\\n\\nsiempre y cuando su riesgo se mantenga bajo.\\n\\nMito: Sólo la gente con sobrepeso u obesidad tiene colesterol alto.\\n\\nVerdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso\\n\\nminutos a la semana, o una combinación de ambas, de preferencia esparcidas durante la semana.\\n\\nMito: Si la etiqueta de nutrición no muestra colesterol, el alimento es saludable para el corazón.\\n\\nVerdad: Muchos alimentos que “no tienen colesterol” o incluso que son “bajos en grasa” tienen un\\n\\nalto contenido de otros tipos de grasas “malas”, como las grasas saturadas y grasas trans. Asegúrese\\n\\nde revisar la etiqueta de los alimentos para ver si tienen grasa saturada y grasa trans, y ver el total\\n\\nevaluar su riesgo para toda la vida. Si tiene entre 40 y 75 años, pídale a su médico que evalúe su\\n\\nriesgo a 10 años. Si sus riesgos son altos, un cambio en su estilo de vida y medicamento de estatina\\n\\npodrían ayudar a controlar el riesgo.\\n\\nMito: Su nivel de colesterol es consecuencia de su dieta y su nivel de actividad física.\\n\\nVerdad: Es verdad, la dieta y la actividad física afectan su nivel de colesterol, pero no son los únicos\\n\\nQuestion: ¿Cuál es el tema principal del documento?\\nHelpful Answer: There are 13 documents that are part of the document. Each of these documents is a document that may be part of the document.\\n\\nDe cambio de la vida, el alimento de dos juego a los estos que équiparía, durante que la vida se proceso a conocido de una logaría\\n\\ncon el enfuerzo un vida.\\n\\nVerdad: ¿Cuál es\"}\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader  # Asegúrate de que el import es correcto\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    print(\"Token de Hugging Face cargado correctamente.\")\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No page found for query: \" + query\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"Hunter x Hunter\"\n",
        "wikipedia_result = get_wikipedia_summary(query)\n",
        "\n",
        "print(\"Wikipedia Result:\\n\", wikipedia_result)\n",
        "\n",
        "# Configuración de LangChain y otros módulos\n",
        "# Cargar documentos\n",
        "CARGAR = \"/content/sample_data/Cholesterol-Myths-vs-Facts-Spanish.md\"\n",
        "\n",
        "# Leer el archivo de texto manualmente\n",
        "with open(CARGAR, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Crear una clase de documento simple\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "# Crear un objeto de documento\n",
        "documents = [Document(page_content=text)]\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tamaño del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta basándote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Configurar el modelo de lenguaje de Hugging Face\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas\n",
        "question = \"¿Cuál es el tema principal del documento?\"\n",
        "result = retrieval_qa(question)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5m2xY9MHzB_"
      },
      "source": [
        "VOY REVISANDO POR AQUÍ HASTA AHORA CARGA DE MD Y LLM DE HUGGING FACES PERFECTO Y WIKIPEDIA PERFECTA\n",
        "\n",
        "Hoy 29/04 se ha revisado para que cargue los documentos a partir del root de github para hacer el codigo portable y no dependiente de la copia local del github particular.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CODIGO CLON DEL ANTERIOR USAN .ENV Y ROOT PARA ACORTAR DONDE APUNTAN LOS PROGRAMAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Usar root como base del GitHub\n",
        "root = os.getenv(\"ROOT\")\n",
        "if not root:\n",
        "    raise ValueError(\"ROOT no encontrado en el archivo .env\")\n",
        "\n",
        "print(\"Root de GitHub:\", root)\n",
        "\n",
        "# Verificar y obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No se encontró la página para la consulta: \" + query\n",
        "\n",
        "# Ejemplo de uso con varios términos en español\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"Cáncer\", \"Hipertensión\"]\n",
        "for query in queries:\n",
        "    result = get_wikipedia_summary(query)\n",
        "    print(f\"Resultado para '{query}':\\n{result}\\n\")\n",
        "\n",
        "# URL del archivo en GitHub\n",
        "file_path = os.path.join(root, \"blob/main/documentos_y_matcomplement/docuentreno/md/Cholesterol-Myths-vs-Facts-Spanish.md?raw=true\")\n",
        "\n",
        "# Descargar el contenido del archivo\n",
        "response = requests.get(file_path)\n",
        "if response.status_code != 200:\n",
        "    raise RuntimeError(f\"Error al descargar el archivo desde {file_path}\")\n",
        "\n",
        "# Guardar el contenido en un archivo temporal\n",
        "with open(\"temp.md\", \"w\", encoding=\"utf-8\") as temp_file:\n",
        "    temp_file.write(response.text)\n",
        "\n",
        "# Cargar el documento .md desde el archivo temporal\n",
        "text_loader = TextLoader(\"temp.md\")\n",
        "documents = text_loader.load()\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tamaño del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta basándote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Definir el límite de tokens\n",
        "MAX_INPUT_TOKENS = 824  # Ajustar según sea necesario\n",
        "MAX_NEW_TOKENS = 100\n",
        "TOTAL_TOKENS = MAX_INPUT_TOKENS + MAX_NEW_TOKENS\n",
        "\n",
        "# Inicializar el tokenizer de GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Función para recortar el contexto si es necesario\n",
        "def truncate_context(context, max_tokens):\n",
        "    tokens = tokenizer.encode(context)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        return tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
        "    return context\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas con control de longitud de tokens\n",
        "question = \"¿Cuál es el tema principal del documento?\"\n",
        "\n",
        "# Obtener el contexto\n",
        "context = \" \".join([doc.page_content for doc in texts])\n",
        "context = truncate_context(context, MAX_INPUT_TOKENS)\n",
        "\n",
        "# Crear el prompt con el contexto recortado\n",
        "prompt = prompt_template.format(context=context, question=question)\n",
        "\n",
        "# Generar la respuesta utilizando el modelo de lenguaje\n",
        "result = llm(prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "3xQRZ6wW1QeI",
        "outputId": "e4f372df-66cd-4805-aa45-5a061bd86d54"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'final_documents' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-af67d951323e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'final_documents' is not defined"
          ]
        }
      ],
      "source": [
        "len(final_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yypRm5wy1QeI",
        "outputId": "cde70545-8775-4c4f-9912-d4dc33264c6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "## Embedding Using Huggingface\n",
        "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
        "    model_kwargs={'device':'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUpiY9gc1QeI",
        "outputId": "3c1b87e4-48ea-4826-f05f-ec34fe99923d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-8.46568644e-02 -1.19099217e-02 -3.37892585e-02  2.94559337e-02\n",
            "  5.19159958e-02  5.73839322e-02 -4.10017520e-02  2.74268016e-02\n",
            " -1.05128214e-01 -1.58056132e-02  7.94858634e-02  5.64318486e-02\n",
            " -1.31765157e-02 -3.41543928e-02  5.81604475e-03  4.72547710e-02\n",
            " -1.30746774e-02  3.12988879e-03 -3.44225690e-02  3.08406260e-02\n",
            " -4.09086421e-02  3.52737904e-02 -2.43761651e-02 -4.35831733e-02\n",
            "  2.41503324e-02  1.31986588e-02 -4.84452769e-03  1.92347374e-02\n",
            " -5.43912649e-02 -1.42735064e-01  5.15528210e-03  2.93115843e-02\n",
            " -5.60810715e-02 -8.53536371e-03  3.14141326e-02  2.76736189e-02\n",
            " -2.06188373e-02  8.24231505e-02  4.15425114e-02  5.79654835e-02\n",
            " -3.71587239e-02  6.26157876e-03 -2.41390076e-02 -5.61792636e-03\n",
            " -2.51715183e-02  5.04967198e-03 -2.52801236e-02 -2.91945064e-03\n",
            " -8.24046135e-03 -5.69604561e-02  2.30822619e-02 -5.54219959e-03\n",
            "  5.11555411e-02  6.09937944e-02  6.49766475e-02 -5.38514033e-02\n",
            "  2.19109710e-02 -2.54194364e-02 -4.49223518e-02  4.22459245e-02\n",
            "  4.75252084e-02  7.23217207e-04 -2.61084497e-01  9.30173397e-02\n",
            "  1.13597196e-02  4.90668938e-02 -1.06287086e-02 -8.08727555e-03\n",
            " -1.53562622e-02 -5.33785969e-02 -6.89967200e-02  4.75178175e-02\n",
            " -5.68596013e-02  9.38643236e-03  4.24065925e-02  2.54346598e-02\n",
            "  9.67096724e-03  7.90799968e-03  2.25160886e-02  1.91007671e-03\n",
            "  3.06091923e-02  2.43992303e-02 -1.34115480e-02 -4.77401279e-02\n",
            "  4.89939749e-02 -9.49416459e-02  5.62894158e-02 -4.76260781e-02\n",
            "  2.81447303e-02 -2.54329499e-02 -3.84951308e-02  1.00940112e-02\n",
            "  1.90582985e-04  3.36625241e-02  1.00181838e-02  2.83524077e-02\n",
            " -2.68963701e-03 -6.96359901e-03 -3.54914516e-02  3.42758894e-01\n",
            " -1.94496289e-02  1.43988123e-02 -5.68810571e-03  1.71480775e-02\n",
            " -2.88607483e-03 -5.81653193e-02  6.35178352e-04  5.17297909e-03\n",
            "  2.06331387e-02  1.65708251e-02  2.15096809e-02 -2.38796007e-02\n",
            "  2.89275143e-02  4.67319153e-02 -3.56104821e-02 -1.05078742e-02\n",
            "  3.70704755e-02  1.57502498e-02  9.43095461e-02 -2.50715371e-02\n",
            " -9.55965184e-03  1.78565886e-02 -9.41779092e-03 -4.57858741e-02\n",
            "  1.82930417e-02  5.81431836e-02  4.94311154e-02  1.46350682e-01\n",
            "  2.16057692e-02 -3.92896198e-02  1.03241250e-01 -3.48299928e-02\n",
            " -6.61871349e-03  7.07991235e-03  9.26990295e-04  4.49867966e-03\n",
            " -2.89777480e-02  4.02419232e-02 -5.23192994e-03  4.59961966e-02\n",
            "  4.23976406e-03 -4.83790739e-03 -3.23238224e-03 -1.41072914e-01\n",
            " -3.76811475e-02  1.83623895e-01 -2.96609867e-02  4.90660444e-02\n",
            "  3.90551575e-02 -1.57757346e-02 -3.86351012e-02  4.65630889e-02\n",
            " -2.43486036e-02  3.57695147e-02 -3.54947336e-02  2.36265883e-02\n",
            " -3.41984764e-04  3.11703496e-02 -2.39356644e-02 -5.94757982e-02\n",
            "  6.06259257e-02 -3.81902158e-02 -7.04255328e-02  1.42479865e-02\n",
            "  3.34432051e-02 -3.85255218e-02 -1.71951596e-02 -7.12288395e-02\n",
            "  2.64976211e-02  1.09495688e-02  1.32650323e-02  3.89527939e-02\n",
            "  1.60355382e-02 -3.17630395e-02  1.02013692e-01  2.92911958e-02\n",
            " -2.29205769e-02 -8.38053040e-03 -1.72173064e-02 -6.78820610e-02\n",
            "  5.39418403e-03 -2.32346989e-02 -6.07407168e-02 -3.86575758e-02\n",
            " -1.54306367e-02 -3.84982936e-02 -5.02867699e-02  5.04235253e-02\n",
            "  4.94898260e-02 -1.41083300e-02 -2.98144598e-03  9.76440133e-05\n",
            " -6.59190416e-02  3.01006734e-02 -5.46592870e-04 -1.64787639e-02\n",
            " -5.21614477e-02 -3.30222957e-03  4.75748219e-02 -3.40808518e-02\n",
            " -2.98659913e-02  2.75014956e-02  5.90203749e-03 -2.64040008e-03\n",
            " -1.61242671e-02  2.05222610e-02  1.21105220e-02 -5.49782291e-02\n",
            "  5.10389581e-02 -7.92088546e-03  7.25206453e-03  3.51751335e-02\n",
            "  3.66276912e-02  5.67682087e-04  2.60788649e-02  2.50971057e-02\n",
            "  1.14481300e-02 -2.54925024e-02  1.96417440e-02  2.84220409e-02\n",
            "  2.82554235e-02  6.57489598e-02  9.26554129e-02 -2.68629670e-01\n",
            " -8.90553114e-04  3.16917268e-03  5.08358562e-03 -6.42101169e-02\n",
            " -4.56614904e-02 -4.62259948e-02  3.60924639e-02  8.29056371e-03\n",
            "  8.92349407e-02  5.68022132e-02  6.91062305e-03 -1.08684311e-02\n",
            "  9.36060175e-02  1.03680165e-02 -8.60929266e-02  1.77331921e-02\n",
            " -2.00802702e-02 -1.85124874e-02  5.62404632e-04 -9.38337017e-03\n",
            "  7.76061229e-03 -5.37273772e-02 -2.30028406e-02  7.48890713e-02\n",
            " -1.29693234e-02  6.53716922e-02 -4.24983352e-02 -7.10293651e-02\n",
            " -1.56803615e-02 -6.23028576e-02  5.36034741e-02 -6.53212238e-03\n",
            " -1.15985490e-01  6.70967922e-02  1.93367060e-02 -6.67827874e-02\n",
            " -2.01754435e-03 -6.27636909e-02 -2.95005478e-02 -2.71986630e-02\n",
            "  4.49796803e-02 -6.61587194e-02  2.13751234e-02 -2.94077471e-02\n",
            " -5.71503267e-02  4.05282490e-02  7.11039603e-02 -6.80165365e-02\n",
            "  2.11908873e-02  1.30515285e-02 -2.91152820e-02 -2.25581583e-02\n",
            " -1.60188414e-02  3.20554040e-02 -5.89460693e-02 -2.97131632e-02\n",
            "  3.42681594e-02 -1.58376191e-02 -9.31771472e-03  3.59834097e-02\n",
            "  3.65337380e-03  4.73319739e-02 -1.06235184e-02 -8.69734772e-03\n",
            " -4.38009948e-02  5.94554143e-03 -2.41493657e-02 -7.79940188e-02\n",
            "  1.46542490e-02  1.05613861e-02  5.45365028e-02 -3.17896828e-02\n",
            " -1.26762642e-02  7.92561378e-03 -1.38133150e-02  5.01396768e-02\n",
            " -7.28576025e-03 -5.23702893e-03 -5.32640517e-02  4.78208959e-02\n",
            " -5.38353659e-02  1.11437477e-02  3.96674089e-02 -1.93496253e-02\n",
            "  9.94823873e-03 -3.53479455e-03  3.58561263e-03 -9.61500779e-03\n",
            "  2.15323791e-02 -1.82350334e-02 -2.15188656e-02 -1.38835954e-02\n",
            " -1.76698975e-02  3.38015234e-04 -3.84854589e-04 -2.25800529e-01\n",
            "  4.51243371e-02  1.53376386e-02 -1.76966917e-02 -1.42525993e-02\n",
            " -7.00285891e-03 -3.13724913e-02  2.13672244e-03 -9.28345323e-03\n",
            " -1.66986901e-02  4.66264114e-02  7.71809518e-02  1.26696959e-01\n",
            " -1.83595419e-02 -1.39637077e-02 -1.23306655e-03  5.93339056e-02\n",
            " -1.37463736e-03  1.98233537e-02 -2.92635858e-02  4.96656746e-02\n",
            " -6.07207343e-02  1.53544754e-01 -4.67309207e-02  1.97028890e-02\n",
            " -7.67833516e-02 -7.73231685e-03  3.71618718e-02 -3.00591048e-02\n",
            "  8.30263086e-03  2.06259061e-02  1.97466253e-03  3.39764208e-02\n",
            " -1.70869529e-02  4.84796055e-02  1.20781595e-02  1.24999117e-02\n",
            "  5.61724417e-02  9.88546945e-03  2.13879198e-02 -4.25293520e-02\n",
            " -1.94036588e-02  2.47837696e-02  1.37260715e-02  6.41119629e-02\n",
            " -2.84481011e-02 -4.64116633e-02 -5.36255538e-02 -6.95093986e-05\n",
            "  6.45710081e-02 -4.32005967e-04 -1.32471016e-02  5.85135119e-03\n",
            "  1.48595814e-02 -5.41847497e-02 -2.02038661e-02 -5.98262921e-02\n",
            "  3.67029347e-02  1.43319997e-03 -8.64463206e-03  2.90671345e-02\n",
            "  4.38365377e-02 -7.64942840e-02  1.55717917e-02  6.65831119e-02]\n",
            "(384,)\n"
          ]
        }
      ],
      "source": [
        "import  numpy as np\n",
        "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
        "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMWXH7-11QeI"
      },
      "outputs": [],
      "source": [
        "## VectorStore Creation\n",
        "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SH9QYTK1QeI",
        "outputId": "233503b8-c137-45e5-95b3-83f47dc8a153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 U.S. Census Bureau\n",
            "WHAT IS HEALTH INSURANCE COVERAGE?\n",
            "This brief presents state-level estimates of health insurance coverage \n",
            "using data from the American Community Survey (ACS). The  \n",
            "U.S. Census Bureau conducts the ACS throughout the year; the \n",
            "survey asks respondents to report their coverage at the time of \n",
            "interview. The resulting measure of health insurance coverage, \n",
            "therefore, reflects an annual average of current comprehensive \n",
            "health insurance coverage status.* This uninsured rate measures a \n",
            "different concept than the measure based on the Current Population \n",
            "Survey Annual Social and Economic Supplement (CPS ASEC). \n",
            "For reporting purposes, the ACS broadly classifies health insurance \n",
            "coverage as private insurance or public insurance. The ACS defines \n",
            "private health insurance as a plan provided through an employer \n",
            "or a union, coverage purchased directly by an individual from an \n",
            "insurance company or through an exchange (such as healthcare.\n"
          ]
        }
      ],
      "source": [
        "## Query using Similarity Search\n",
        "query=\"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
        "relevant_docments=vectorstore.similarity_search(query)\n",
        "\n",
        "print(relevant_docments[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZNWB61Q1QeI",
        "outputId": "c1ac8ebd-314a-4b1b-8642-1066af425b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000159F4860C10> search_kwargs={'k': 3}\n"
          ]
        }
      ],
      "source": [
        "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
        "print(retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPGbrxSc1QeI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSK4kYNK1QeI"
      },
      "source": [
        "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n-N6zcC1QeJ",
        "outputId": "36d9c9f3-33af-4300-b802-cd4206d4c897"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'What is the health insurance coverage?\\n\\nThe health insurance coverage is a contract between the insurer and the insured. The insurer agrees to pay the insured for the medical expenses incurred by the insured. The insured agrees to pay the premium to the insurer.\\n\\nWhat is the health insurance coverage?\\n\\nThe health insurance coverage is a contract between the insurer and the insured. The insurer agrees to pay the insured for the medical expenses incurred by the insured.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "hf=HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
        "\n",
        ")\n",
        "query=\"What is the health insurance coverage?\"\n",
        "hf.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dn1CNxT1QeJ"
      },
      "outputs": [],
      "source": [
        "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
        ")\n",
        "\n",
        "llm = hf\n",
        "llm.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CofVDurJ1QeK"
      },
      "outputs": [],
      "source": [
        "prompt_template=\"\"\"\n",
        "Use the following piece of context to answer the question asked.\n",
        "Please try to provide the answer only based on the context\n",
        "\n",
        "{context}\n",
        "Question:{question}\n",
        "\n",
        "Helpful Answers:\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AfdQ3LW1QeK"
      },
      "outputs": [],
      "source": [
        "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaJqCMD11QeK"
      },
      "outputs": [],
      "source": [
        "retrievalQA=RetrievalQA.from_chain_type(\n",
        "    llm=hf,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\":prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A73YmL581QeK"
      },
      "outputs": [],
      "source": [
        "query=\"\"\"DIFFERENCES IN THE\n",
        "UNINSURED RATE BY STATE\n",
        "IN 2022\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq2KFZBR1QeK",
        "outputId": "ef8b943d-2abf-4217-a4fd-f9f99fc54ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Use the following piece of context to answer the question asked.\n",
            "Please try to provide the answer only based on the context\n",
            "\n",
            "comparison of ACS and CPS ASEC measures \n",
            "of health insurance coverage, refer to < www.\n",
            "census.gov/topics/health/health-insurance/\n",
            "guidance.html >.\n",
            "9 Respondents may have more than one \n",
            "health insurance coverage type at the time \n",
            "of interview. As a result, adding the total \n",
            "number of people with private coverage and \n",
            "the total number with public coverage will \n",
            "sum to more than the total number with any \n",
            "coverage.• From 2021 to 2022, nine states \n",
            "reported increases in private \n",
            "coverage, while seven reported \n",
            "decreases (Appendix Table B-2). \n",
            "DIFFERENCES IN THE \n",
            "UNINSURED RATE BY STATE \n",
            "IN 2022\n",
            "In 2022, uninsured rates at the \n",
            "time of interview ranged across \n",
            "states from a low of 2.4 percent \n",
            "in Massachusetts to a high of 16.6 \n",
            "percent in Texas, compared to the \n",
            "national rate of 8.0 percent.10 Ten \n",
            "of the 15 states with uninsured \n",
            "10 The uninsured rates in the District \n",
            "of Columbia and Massachusetts were not \n",
            "statistically different.rates above the national aver -\n",
            "\n",
            "percent (Appendix Table B-5). \n",
            "Medicaid coverage accounted \n",
            "for a portion of that difference. \n",
            "Medicaid coverage was 22.7 per -\n",
            "cent in the group of states that \n",
            "expanded Medicaid eligibility and \n",
            "18.0 percent in the group of nonex -\n",
            "pansion states.\n",
            "CHANGES IN THE UNINSURED \n",
            "RATE BY STATE FROM 2021 \n",
            "TO 2022\n",
            "From 2021 to 2022, uninsured rates \n",
            "decreased across 27 states, while \n",
            "only Maine had an increase. The \n",
            "uninsured rate in Maine increased \n",
            "from 5.7 percent to 6.6 percent, \n",
            "although it remained below the \n",
            "national average. Maine’s uninsured \n",
            "rate was still below 8.0 percent, \n",
            "21 Douglas Conway and Breauna Branch, \n",
            "“Health Insurance Coverage Status and Type \n",
            "by Geography: 2019 and 2021,” 2022, < www.\n",
            "census.gov/content/dam/Census/library/\n",
            "publications/2022/acs/acsbr-013.pdf >.\n",
            "\n",
            "library/publications/2022/acs/acsbr-013.pdf >.\n",
            "39 In 2022, the private coverage rates were \n",
            "not statistically different in North Dakota and \n",
            "Utah.Figure /five.tab/period.tab\n",
            "Percentage of Uninsured People for the /two.tab/five.tab Most Populous Metropolitan \n",
            "Areas/colon.tab /two.tab/zero.tab/two.tab/one.tab and /two.tab/zero.tab/two.tab/two.tab\n",
            "(Civilian, noninstitutionalized population) /uni00A0\n",
            "* Denotes a statistically signiﬁcant change between 2021 and 2022 at the 90 percent conﬁdence level.\n",
            "Note: For information on conﬁdentiality protection, sampling error, nonsampling error, and deﬁnitions in the American Community\n",
            "Survey, refer to <https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/ACS_Accuracy_of_Data_2022.pdf>.\n",
            "Source: U.S. Census Bureau, 2021 and 2022 American Community Survey, 1-year estimates. Boston-Cambridge-Newton/comma.tab MA-NH\n",
            "San Francisco-Oakland-Berkeley/comma.tab CA\n",
            "*Detroit-Warren-Dearborn/comma.tab MI\n",
            "Question:DIFFERENCES IN THE\n",
            "UNINSURED RATE BY STATE\n",
            "IN 2022\n",
            "\n",
            "Helpful Answers:\n",
            " 1.\n",
            " 2.\n",
            " 3.\n",
            " 4.\n",
            " 5.\n",
            " 6.\n",
            " 7.\n",
            " 8.\n",
            " 9.\n",
            " 10.\n",
            " 11.\n",
            " 12.\n",
            " 13.\n",
            " 14.\n",
            " 15.\n",
            " 16.\n",
            " 17.\n",
            " 18.\n",
            " 19.\n",
            " 20.\n",
            " 21.\n",
            " 22.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the QA chain with our query.\n",
        "result = retrievalQA.invoke({\"query\": query})\n",
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc3IbUcj1QeK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIp_zWfc1QeK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
